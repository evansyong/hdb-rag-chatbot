{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afaa622-c96e-46c7-97ac-498b9056c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evans\\OneDrive\\Documents\\Competitions\\National AI Student Challenge 2025 - Huawei Track\\hdb-rag-chatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import re\n",
    "import gradio as gr\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed63f3a-1d7a-47c7-86b1-36f5d5fdbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file into LangChain\n",
    "loader = TextLoader(\"hdb_faqs.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae821e3f-819d-4e10-9644-d38e41c41dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0549fc5f-2a4b-4abc-969e-1bd86a5b3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evans\\AppData\\Local\\Temp\\ipykernel_30776\\1029300437.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding_function = OllamaEmbeddings(model=\"deepseek-r1\")  # by default, 7B parameter model\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama embeddings using DeepSeek-R1\n",
    "embedding_function = OllamaEmbeddings(model=\"deepseek-r1\")  # by default, 7B parameter model\n",
    "# Parallelize embedding generation\n",
    "def generate_embedding(chunk):\n",
    "    return embedding_function.embed_query(chunk.page_content)\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    embeddings = list(executor.map(generate_embedding, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96dad29-77be-4f71-9fc8-d85810ff4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client and create/reset the collection\n",
    "client = Client(Settings())\n",
    "collection = client.create_collection(name=\"hdb_faqs\")\n",
    "# Add documents and embeddings to Chroma\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk.page_content], \n",
    "        metadatas=[{'id': idx}], \n",
    "        embeddings=[embeddings[idx]], \n",
    "        ids=[str(idx)]  # Ensure IDs are strings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65436941-5c17-4d62-b32f-ca199d9fcaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever using Ollama embeddings for queries\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_function)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fece619-bfe0-48ec-858d-a94b90b12c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question):\n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.invoke(question)\n",
    "    # Combine the retrieved content\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea66d890-98da-4896-81d7-6c2e60ec8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(question, context):\n",
    "    # Format the input prompt\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    # Query DeepSeek-R1 using Ollama\n",
    "    response = ollama.chat(\n",
    "        model=\"deepseek-r1\",\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]\n",
    "    )\n",
    "    # Clean and return the response\n",
    "    response_content = response['message']['content']\n",
    "    final_answer = re.sub(r'<think>.*?</think>', '', response_content, flags=re.DOTALL).strip()\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca36fddb-122c-4a5f-aeca-2f6b780ff09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    # Retrieve context and generate an answer using RAG\n",
    "    context = retrieve_context(question)\n",
    "    answer = query_deepseek(question, context)\n",
    "    return answer\n",
    "# Set up the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Low-Fidelity RAG Chatbot\",\n",
    "    description=\"Ask any question regarding HDB Municipal Services!\"\n",
    ")\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
